To get better with ml, a wonderful place to start is by doing kaggle's competitions. 
A quote that brings up frequently to motivate my time spent on practice vs reading is the following from Art and Fear.
> The ceramics teacher announced on opening day that he was dividing the class into two groups. All those on the left side of the studio, he said, would be graded solely on the quantity of work they produced, all those on the right solely on its quality. His procedure was simple: on the final day of class he would bring in his bathroom scales and weigh the work of the “quantity” group: fifty pounds of pots rated an “A”, forty pounds a “B”, and so on. Those being graded on “quality”, however, needed to produce only one pot —albeit a perfect one —to get an “A”. Well, came grading time and a curious fact emerged: the works of highest quality were all produced by the group being graded for quantity. It seems that while the “quantity” group was busily churning out piles of work—and learning from their mistakes —the “quality” group had sat theorizing about perfection, and in the end had little more to show for their efforts than grandiose theories and a pile of dead clay.

Recently mlt organized a meetup with Philipp singer, he gaves a lot of insight about his experience, one that can only express after doing tons of competitions. At the same time, I learned that there is a title for one that is the best of the best in Kaggle, which is Kaggle Grandmaster. It reminds me of the recently viewed serie Queen's Gambit from Netflix. It seems fun to pursuit this title, I'll give it a run. I'll be updating my list of references here also.

My journey to Kaggle #1 with Philipp Singer - Philipp Singer - https://www.youtube.com/watch?v=OenmJTdF0-M

Progressively approaching Kaggle - Rohan Rao - https://towardsdatascience.com/progressively-approaching-kaggle-f58db71a42a9

What I Have Learnt from My First Kaggle Competition - Fan Ni - https://nifannn.github.io/2017/05/17/first-kaggle/
